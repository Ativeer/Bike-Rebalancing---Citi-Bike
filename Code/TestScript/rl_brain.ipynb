{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is for creating a RL agent class object. This object has the \n",
    "following method:\n",
    "    \n",
    "    1) choose_action: this choose an action based on Q(s,a) and greedy eps\n",
    "    2) learn: this updates the Q(s,a) table\n",
    "    3) check_if_state_exist: this check if a state exist based on env feedback\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class agent():\n",
    "    \n",
    "    \n",
    "    def __init__(self, epsilon, lr, gamma, current_stock, debug, expected_stock, model_based):\n",
    "        \n",
    "        print(\"Created an Agent ...\")\n",
    "        self.actions = [-10, -3, -1, 0]\n",
    "        self.reward = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.debug = debug\n",
    "        self.current_stock = current_stock\n",
    "        self.expected_stock = expected_stock\n",
    "        self.model_based = model_based\n",
    "        \n",
    "        # performance metric\n",
    "        self.q_table = pd.DataFrame(columns = self.actions, dtype = np.float64)\n",
    "        self.hourly_action_history = []\n",
    "        self.hourly_stock_history = []\n",
    "       \n",
    "    def choose_action(self, s, ex):\n",
    "        \n",
    "        '''\n",
    "        This funciton choose an action based on Q Table. It also does \n",
    "        validation to ensure stock will not be negative after moving bikes.\n",
    "        Input: \n",
    "            - s: current bike stock\n",
    "            - ex: expected bike stock in subsequent hour (based on random forests prediction)\n",
    "        \n",
    "        Output:\n",
    "            - action: number of bikes to move\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.check_state_exist(s)\n",
    "        self.current_stock = s\n",
    "        self.expected_stock = ex\n",
    "        \n",
    "        # find valid action based on current stock \n",
    "        # cannot pick an action that lead to negative stock\n",
    "        \n",
    "        # !!!! remove action validation; only rely on reward/penalty !!!\n",
    "        # valid_state_action = self.find_valid_action(self.q_table.loc[s, :])\n",
    "        if self.model_based == True:\n",
    "            #Take an average of current stock and expected stock\n",
    "            try:\n",
    "                avg = int(round(0.5*s + 0.5*ex))\n",
    "            except:\n",
    "                avg = s\n",
    "            self.check_state_exist(avg)\n",
    "            valid_state_action = self.q_table.loc[avg, :]\n",
    "\n",
    "        elif self.model_based == False:\n",
    "            valid_state_action = self.q_table.loc[s, :]\n",
    "                \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "                        \n",
    "            try:\n",
    "                # find the action with the highest expected reward\n",
    "                \n",
    "                valid_state_action = valid_state_action.reindex(np.random.permutation(valid_state_action.index))\n",
    "                action = valid_state_action.idxmax()\n",
    "            \n",
    "            except:\n",
    "                # if action list is null, default to 0\n",
    "                action = 0\n",
    "                        \n",
    "            if self.debug == True:\n",
    "                print(\"Decided to Move: {}\".format(action))\n",
    "                        \n",
    "        else:\n",
    "            \n",
    "            # randomly choose an action\n",
    "            # re-pick if the action leads to negative stock\n",
    "            try:\n",
    "                action = np.random.choice(valid_state_action.index)\n",
    "            except:\n",
    "                action = 0\n",
    "            \n",
    "            if self.debug == True:\n",
    "                print(\"Randomly Move: {}\".format(action))\n",
    "        \n",
    "        self.hourly_action_history.append(action)\n",
    "        self.hourly_stock_history.append(s)\n",
    "        \n",
    "        return action\n",
    " \n",
    "    \n",
    "\n",
    "    def learn(self, s, a, r, s_, ex, g):\n",
    "\n",
    "        \n",
    "        '''\n",
    "        This function updates Q tables after each interaction with the\n",
    "        environment.\n",
    "        Input: \n",
    "            - s: current bike stock\n",
    "            - ex: expected bike stock in next hour\n",
    "            - a: current action (number of bikes to move)\n",
    "            - r: reward received from current state\n",
    "            - s_: new bike stock based on bike moved and new stock\n",
    "        Output: None\n",
    "        '''\n",
    "        \n",
    "        if self.debug == True:\n",
    "            print(\"Moved Bikes: {}\".format(a))\n",
    "            print(\"Old Bike Stock: {}\".format(s))\n",
    "            print(\"New Bike Stock: {}\".format(s_))\n",
    "            print(\"---\")\n",
    "        \n",
    "        self.check_state_exist(s_)\n",
    "\n",
    "        if self.model_based == False:\n",
    "            q_predict = self.q_table.loc[s, a]\n",
    "        elif self.model_based == True:\n",
    "            avg = int(round(0.5*s + 0.5*ex))\n",
    "            self.check_state_exist(avg)\n",
    "            q_predict = self.q_table.loc[avg, a]\n",
    "        \n",
    "\n",
    "        if g == False:\n",
    "            \n",
    "\n",
    "            # Updated Q Target Value if it is not end of day  \n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "        \n",
    "        else:\n",
    "            # Update Q Target Value as Immediate reward if end of day\n",
    "            q_target = r\n",
    "\n",
    "        if self.model_based == False:\n",
    "            self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "        elif self.model_based == True:\n",
    "            self.q_table.loc[avg, a] += self.lr * (q_target - q_predict)\n",
    "        \n",
    "        return\n",
    "\n",
    "    \n",
    "    def check_state_exist(self, state):\n",
    "        \n",
    "        # Add a new row with state value as index if not exist\n",
    "        \n",
    "        if state not in self.q_table.index:\n",
    "            \n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                        [0]*len(self.actions), \n",
    "                        index = self.q_table.columns,\n",
    "                        name = state\n",
    "                        )\n",
    "                )\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "    def find_valid_action(self, state_action):\n",
    "        \n",
    "        '''\n",
    "        This function check the validity acitons in a given state.\n",
    "        Input: \n",
    "            - state_action: the current state under consideration\n",
    "        Output:\n",
    "            - state_action: a pandas Series with only the valid actions that\n",
    "                            will not cause negative stock\n",
    "        '''\n",
    "        \n",
    "        # remove action that will stock to be negative\n",
    "        \n",
    "        for action in self.actions:\n",
    "            if self.current_stock + action < 0:\n",
    "                \n",
    "                if self.debug == True:\n",
    "                    print(\"Drop action {}, current stock {}\".format(action, self.current_stock))\n",
    "                \n",
    "                state_action.drop(index = action, inplace = True)\n",
    "        \n",
    "        return state_action\n",
    "        \n",
    "    \n",
    "    def print_q_table(self):\n",
    "        \n",
    "        print(self.q_table)\n",
    "\n",
    "\n",
    "    def get_q_table(self):\n",
    "        \n",
    "        return self.q_table\n",
    "\n",
    "    \n",
    "    def get_hourly_actions(self):\n",
    "        \n",
    "        return self.hourly_action_history\n",
    "    \n",
    "    def get_hourly_stocks(self):\n",
    "        \n",
    "        return self.hourly_stock_history\n",
    "\n",
    "    \n",
    "    def reset_hourly_history(self):\n",
    "        \n",
    "        self.hourly_action_history = []\n",
    "        self.hourly_stock_history = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
